{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnbvtwukZ7y0",
        "outputId": "e8ab1700-ecfc-4973-c5d8-4514bc79932d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog', '.', 'is', 'very']\n",
            "\n",
            "Bigrams:\n",
            "[('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog'), ('dog', '.'), ('.', 'the'), ('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'is'), ('is', 'very'), ('very', 'quick'), ('quick', '.')]\n",
            "\n",
            "Trigrams:\n",
            "[('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog'), ('lazy', 'dog', '.'), ('dog', '.', 'the'), ('.', 'the', 'quick'), ('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'is'), ('fox', 'is', 'very'), ('is', 'very', 'quick'), ('very', 'quick', '.')]\n",
            "\n",
            "Bigram Frequencies:\n",
            "[(('the', 'quick'), 2), (('quick', 'brown'), 2), (('brown', 'fox'), 2), (('fox', 'jumps'), 1), (('jumps', 'over'), 1), (('over', 'the'), 1), (('the', 'lazy'), 1), (('lazy', 'dog'), 1), (('dog', '.'), 1), (('.', 'the'), 1), (('fox', 'is'), 1), (('is', 'very'), 1), (('very', 'quick'), 1), (('quick', '.'), 1)]\n",
            "\n",
            "Bigram Conditional Frequencies:\n",
            "the: {'quick': 2, 'lazy': 1}\n",
            "quick: {'brown': 2, '.': 1}\n",
            "brown: {'fox': 2}\n",
            "fox: {'jumps': 1, 'is': 1}\n",
            "jumps: {'over': 1}\n",
            "over: {'the': 1}\n",
            "lazy: {'dog': 1}\n",
            "dog: {'.': 1}\n",
            ".: {'the': 1}\n",
            "is: {'very': 1}\n",
            "very: {'quick': 1}\n",
            "\n",
            "Bigram Probabilities:\n",
            "{('the', 'quick'): 0.11764705882352941, ('quick', 'brown'): 0.11764705882352941, ('brown', 'fox'): 0.11764705882352941, ('fox', 'jumps'): 0.058823529411764705, ('jumps', 'over'): 0.058823529411764705, ('over', 'the'): 0.058823529411764705, ('the', 'lazy'): 0.058823529411764705, ('lazy', 'dog'): 0.058823529411764705, ('dog', '.'): 0.058823529411764705, ('.', 'the'): 0.058823529411764705, ('fox', 'is'): 0.058823529411764705, ('is', 'very'): 0.058823529411764705, ('very', 'quick'): 0.058823529411764705, ('quick', '.'): 0.058823529411764705}\n",
            "\n",
            "Next word prediction for 'quick':\n",
            "brown\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('reuters')\n",
        "\n",
        "# Sample text corpus: You can use any text corpus you have\n",
        "text = \"The quick brown fox jumps over the lazy dog. The quick brown fox is very quick.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Unigrams\n",
        "unigrams = list(FreqDist(tokens).keys())\n",
        "print(\"Unigrams:\")\n",
        "print(unigrams)\n",
        "\n",
        "# Bigrams\n",
        "bi_grams = list(bigrams(tokens))\n",
        "print(\"\\nBigrams:\")\n",
        "print(bi_grams)\n",
        "\n",
        "# Trigrams\n",
        "tri_grams = list(trigrams(tokens))\n",
        "print(\"\\nTrigrams:\")\n",
        "print(tri_grams)\n",
        "\n",
        "# Bigram Frequencies\n",
        "bigram_freq = FreqDist(bi_grams)\n",
        "print(\"\\nBigram Frequencies:\")\n",
        "print(bigram_freq.most_common())\n",
        "\n",
        "# Bigram Conditional Frequencies\n",
        "cfd = ConditionalFreqDist((w1, w2) for w1, w2 in bigrams(tokens))\n",
        "print(\"\\nBigram Conditional Frequencies:\")\n",
        "for word in cfd:\n",
        "    print(f\"{word}: {dict(cfd[word])}\")\n",
        "\n",
        "# Bigram Probabilities\n",
        "bigram_probs = {}\n",
        "total_bigrams = len(bi_grams)\n",
        "for (w1, w2), count in bigram_freq.items():\n",
        "    bigram_probs[(w1, w2)] = count / total_bigrams\n",
        "print(\"\\nBigram Probabilities:\")\n",
        "print(bigram_probs)\n",
        "\n",
        "# Next Word Prediction\n",
        "def predict_next_word(word):\n",
        "    if word in cfd:\n",
        "        next_words = cfd[word]\n",
        "        most_common_next_word = next_words.most_common(1)[0][0]\n",
        "        return most_common_next_word\n",
        "    else:\n",
        "        return \"No prediction available\"\n",
        "\n",
        "word_to_predict = \"quick\"\n",
        "predicted_word = predict_next_word(word_to_predict)\n",
        "print(f\"\\nNext word prediction for '{word_to_predict}':\")\n",
        "print(predicted_word)\n"
      ]
    }
  ]
}